{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n<br>\r\n\r\n# Workshop: Microsoft SQL Server Machine Learning Services\r\n\r\n#### <i>A Microsoft Course from the SQL Server team</i>\r\n\r\n## 04 - Phase 2: Data Acquisition and Understanding \r\n\r\n<p style=\"border-bottom: 1px solid lightgrey;\"></p>",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/textbubble.png?raw=true\"><b>Data Acquisition and Understanding</p>\r\n\r\n<br>\r\n<br>\r\n<br>\r\n\r\nYou're learning to use the Team Data Science Process to create a complete solution, using SQL Server as the platform. The phases in the Team Data Science process are:\r\n\r\n<dl>\r\n  <dt>1 - Business Understanding</dt>\r\n  <dt>2 - Data Acquisition and Understanding <i>(This module)</i></dt>\r\n  <dt>3 - Modeling</dt>\r\n  <dt>4 - Deployment</dt>\r\n  <dt>5 - Customer Acceptance and Model Retraining</dt>\r\n<dl>\r\n\r\n<p style=\"border-bottom: 1px solid lightgrey;\"></p>\r\n\r\nFrom Business Intelligence you're familiar with Extract, Transform and Load (ETL) to prepare data for historical, pre-aggregated storage for ad-hoc queries. For Machine Learning, it's more common to extract the data, load it to a source, and then transform the data as late as possible in the process (ELT). This allows the most fidelity within the analysis.\r\n\r\nThere are multiple ways to ingest data, depending on the intended location. For SQL Server, data is often generated within base tables by applications, and other data can be loaded via the bcp program or SQL Server Integration Services. For Azure Machine Learning Services, you can use Azure Blob storage, Azure SQL DB, and many other locations and ingest data using Azure Data Factory and other tools.\r\n\r\n<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/pin.jpg?raw=true\"><b>3.1 Loading Data into the Solution</b></p>\r\n\r\nIn the *Data Acquisition and Understanding* phase of your process you ingest or access data from various locations to answer the questions the organization has asked. In most cases, this data will be in multiple locations. Once the data is ingested into the system, you’ll need to examine it to see what it holds. All data needs cleaning, so after the inspection phase, you’ll replace missing values, add and change columns. You’ll cover more extensive Data Wrangling tasks in other courses. In this section, you’ll use a single Database dataset to train your model.\r\n\r\n### Goals for Data Acquisition and Understanding\r\n\r\n- Produce a clean, high-quality data set whose relationship to the target variables is understood. Locate the data set in the appropriate analytics environment so you are ready to model.\r\n- Develop a solution architecture of the data pipeline that refreshes and scores the data regularly.\r\n\r\n### How to do it\r\n\r\n- Ingest the data into the target analytic environment.\r\n- Explore the data to determine if the data quality is adequate to answer the question.\r\n- Set up a data pipeline to score new or regularly refreshed data.\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/pin.jpg?raw=true\"><b>3.2 Review the Data Ingestion</b>\r\n<br>\r\n\r\nIn this scenario, the data is currently collected into a SQL Server database from the systems on-premises that the marketing team created, for the purpose of reporting and Business Intelligence applications. In the next step, you'll see if it contains enough information for you to perform your analysis.\r\n\r\nYour Source Data was restored to the destination system for operationalization. But for some scenarios, the data may have special security requirements, and in that case, there is one additional step: Anonymizing the data. In the database you have for this assignment, your team has already exported the data, stripping the identifiers from the appropriate locations.\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/checkbox.png?raw=true\"><b>Activity: Anonymizing data</b></p>\r\n\r\n- In the course sample data, the system ID numbers have already been anonymized. Discuss the processes and tools you can use to perform this process, with other options you can think of.\r\n- (Optional) <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/security/sql-data-discovery-and-classification?view=sql-server-ver15\r\n\" target=\"_blank\">Review the following article on Data Classification and apply it to the database</a>. Are there other security measures you should take?\r\n- (Optional) <a href=\"https://docs.microsoft.com/en-us/sql/t-sql/statements/add-sensitivity-classification-transact-sql?view=azuresqldb-current&viewFallbackFrom=sql-server-ver15\" target=\"_blank\">Review the following article to learn how to apply the classification in the database, rather than using a tools-centric approach</a>.\r\n\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/checkbox.png?raw=true\"><b>Activity: The Training environment </b></p>\r\n\r\nFor the Machine Learning section of this course, you will use the R language with various Microsoft Machine Learning libraries for training a model. You'll run the model training in your SQL Server 2019 Instance - sometimes called the \"Training Target\". There are many considerations for this decision, and in production you are concerned with several of these factors.\r\n\r\n- As a class, list the reasons and justifications for using a local system for the model training\r\n- When would it be appropriate to use a remote system for the training effort?\r\n- What concerns have not been addressed so far in the architecture you are currently using?\r\n\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/checkbox.png?raw=true\"><b>Activity: Using Other Data Sources</b></p>\r\n\r\nIn this activity, you'll think about the data you need to perform an accurate analysis and clustering exercise.\r\n\r\n*Note that you will create an environment to isolate and stay consistent with the Data Scientist's original experiment. This is a rather complicated problem, and the course uses far too few data sources and considerations, due to time and explainability constraints.*\r\n\r\nIn your ADS documentation, answer the following questions:\r\n\r\n- How many groups of people would be useful to the business?\r\n- What features do you think you need to know to group people adequately, given the purpose of the question? (The *Returns* behavior)\r\n- Are more features or fewer features better for this task?\r\n\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/pin.jpg?raw=true\"><b>3.2 Data Exploration and Profiling</b></p>\r\n\r\nWith the data located and loaded, you can now begin the exploration. You need to know the \"shape\" of the data, some basic statistics, and very importantly, any features you can use as well as those you still need.\r\n\r\nYou can use various libraries and language statements for a majority of the exploration. The SQL language has a rich, declarative structure that will provide most of the information you need. There are other options for exploring your data, such as R or Python. R is a data-first language, and most Data Scientists are familiar with using it to explore data. Python has a rich set of libraries to query, visualize and explore data.\r\n\r\nYou can perform this analysis remotely on your local system, or in Azure, using Notebooks as you will in this course. You can also use SQL Server Stored Procedures to hold the Python or R code and run it within SQL Server ML Services as you saw in the previous module. You can  use a series of R or Python Library calls to query the data held in SQL Server and work with it locally to a workstation in a traditional fashion.\r\n\r\nIn the graphic below, the Data Scientist works with R locally, and once they determine a good model, deploy that to SQL Server. Clients use that Model by calling a standard SQL Server Stored Procedure, and no R client is needed on their machine or device:\r\n\r\n<p>\r\n<img src=\"https://github.com/microsoft/sqlworkshops/blob/master/graphics/MLServerArchitecture.png?raw=TRUE\" width=\"500\">\r\n<p>\r\n\r\nUsing this scenario, you could create the model completely in R locally, and then deploy the model to SQL Server for scoring. This is a common practice, so that the Data Scientist can work quickly for debugging purposes.\r\n\r\nIn this course, however, we'll run all of the R code in a stored procedure using the Extensibility Framework. One advantage for staying completely within SQL Server is that you have both R and T-SQL available for data exploration.\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/checkbox.png?raw=true\"><b>Activity: Explore the Data using Transact-SQL</b></p>\r\n\r\n- Run the cells below to explore the data. We'll be focusing on two tables in our database.\r\n- Add a `Code` cell in this Notebook to show all of the tables in the database, and select the columns in the ones you think might also be helpful.\r\n- What fields do you think we need for this solution? Do we have them all? In which tables?\r\n\r\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "USE Analysis;\nGO\nSELECT TOP (10) * FROM [dbo].[store_sales];\nGO\nSELECT TOP (10) * FROM [dbo].[store_returns];\nGO",
            "metadata": {},
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/thinking.jpg?raw=true\"><b>For Further Study</b></p>\r\n\r\n<br>\r\n\r\n- Data Acquisition and Understand Reference: https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-data\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/education1.png?raw=true\"><b>Next</b>: Phase 3 - Modeling</p>\r\n\r\nNext, you'll continue working through the Team Data Science Process in the next phase - *05 - Phase 3 - Modeling*. Open that Notebook to continue.",
            "metadata": {}
        }
    ]
}